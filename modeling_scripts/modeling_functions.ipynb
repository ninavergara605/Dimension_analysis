{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modeling_functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPWPBFLGXY/AHS7Ha6EZ+s"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtLOufVRUdYW"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, cross_validate\n",
        "from sklearn.metrics import  confusion_matrix, plot_roc_curve\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDON9a5hONBw"
      },
      "source": [
        "### Presentation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q3PerxQdZ6E"
      },
      "source": [
        "def print_scores(cv_results, best_idx = None):\n",
        "  '''\n",
        "  Prints out the scoring metrics generated from cross_validate or GridSearchCV functions \n",
        "  --------------------------------------------------------------------------------------\n",
        "  cv_results: Dict of scoring metrics.\n",
        "  best_idx: Specifies the index of the best model scores.\n",
        "\n",
        "  '''\n",
        "  if 'mean_test_accuracy' in cv_results.keys():\n",
        "    # mean_test_accuracy is a GridSearchCV key, so use the index of the best model for each metric\n",
        "    test_accuracy, train_accuracy = cv_results['mean_test_accuracy'][best_idx], cv_results['mean_train_accuracy'][best_idx]\n",
        "    test_f1, train_f1 = cv_results['mean_test_f1'][best_idx], cv_results['mean_train_f1'][best_idx] \n",
        "    test_auc, train_auc = cv_results['mean_test_roc_auc'][best_idx], cv_results['mean_train_roc_auc'][best_idx]\n",
        "  else:\n",
        "    # Metrics were generated from cross_validate, so take the mean of each array for average performance\n",
        "    test_accuracy, train_accuracy  = np.mean(cv_results['test_accuracy']), np.mean(cv_results['train_accuracy'])\n",
        "    test_f1, train_f1 = np.mean(cv_results['test_f1']), np.mean(cv_results['train_f1'])\n",
        "    test_auc, train_auc  = np.mean(cv_results['test_roc_auc']), np.mean(cv_results['train_roc_auc'])\n",
        "     \n",
        "\n",
        "  print(f\"Average accuracy-- test: {test_accuracy}, train: {train_accuracy}\")\n",
        "  print(f\"Average f1-- test: {test_f1}, train: {train_f1}\")\n",
        "  print(f\"Average roc_auc-- test roc_auc: {test_auc}, train: {train_auc}\")\n",
        "\n",
        "def create_plots(model, X, y_true):\n",
        "  '''\n",
        "  Plots the confusion matrix and ROC curve on the same figure.\n",
        "  ------------------------------------------------------------\n",
        "  model: Fitted model\n",
        "  X: Values to make predictions from\n",
        "  y_true: Actual y values\n",
        "  '''\n",
        "  fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,4))\n",
        "\n",
        "  y_pred = model.predict(X)\n",
        "  conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "  sns.heatmap(conf_matrix, annot=True, ax=ax1)\n",
        "\n",
        "  plot_roc_curve(model,X,y_true, ax=ax2)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ImUPMwLd1o4"
      },
      "source": [
        "### Cross-Validation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zmhk521OLF7"
      },
      "source": [
        "def grid_search_cv(model, grid, X,y, cv=5):\n",
        "    scoring_metrics = ['accuracy', 'f1', 'roc_auc']\n",
        "    gs = GridSearchCV(estimator=model, param_grid=grid, cv=cv, return_train_score=True, scoring=scoring_metrics, refit='f1')\n",
        "    gs.fit(X, y)\n",
        "    \n",
        "    print_scores(gs.cv_results_, gs.best_index_)\n",
        "    create_plots(gs, X,y)\n",
        "    return gs.best_params_\n",
        "\n",
        "def evaluate_model(model, X, y, cv=4):\n",
        "  scoring_metrics = ['accuracy', 'f1', 'roc_auc']\n",
        "  cv_results = cross_validate(model, X, y, cv=cv, return_train_score=True, scoring=scoring_metrics)\n",
        "  print_scores(cv_results)\n",
        "  \n",
        "  model.fit(X,y)\n",
        "  create_plots(model, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}